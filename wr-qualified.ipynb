{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from moztelemetry.dataset import Dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Dataset API to fetch data.  Documentation can be found at: https://python-moztelemetry.readthedocs.io/en/stable/api.html#dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this example is to plot the startup distribution for each OS. Let's see how many parallel workers we have at our disposal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the schema of the dataset we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'submissionDate',\n",
       " u'sourceName',\n",
       " u'sourceVersion',\n",
       " u'docType',\n",
       " u'appName',\n",
       " u'appUpdateChannel',\n",
       " u'appVersion',\n",
       " u'appBuildId']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.from_source('telemetry').schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Dataset of Telemetry submissions for a given submission date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pings_dataset = (\n",
    "    Dataset.from_source('telemetry')\n",
    "    .where(docType='main')\n",
    "    #.where(appBuildId='20180721100146')\n",
    "    .where(submissionDate='20180820')\n",
    "    .where(appUpdateChannel=\"release\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the properties we need and then take a 10% sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: THIS IS NOT A REPRESENTATIVE SAMPLE.\n",
      "This 'sampling' is based on s3 files and is highly\n",
      "susceptible to skew. Use only for quicker performance\n",
      "while prototyping.\n",
      "fetching 8878.45489MB in 1672 files...\n"
     ]
    }
   ],
   "source": [
    "pings = (\n",
    "    pings_dataset\n",
    "    .select(\n",
    "        'clientId',\n",
    "        buildId='application.buildId',\n",
    "        experiments='environment.experiments',\n",
    "        os='environment.system.os',\n",
    "        gfx='environment.system.gfx')\n",
    "    .records(sc, sample=0.01)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pings = (\n",
    "#    pings_dataset\n",
    "#    .records(sc, sample=0.01)\n",
    "#)\n",
    "#pings.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2272516"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'buildId': u'20180807170231',\n",
       "  'checkerboardin': None,\n",
       "  'clientId': u'313521b7-7dd8-4a2f-86aa-c80b65bfcb34',\n",
       "  'composite_time': None,\n",
       "  'content_paint': None,\n",
       "  'content_paint_phase': None,\n",
       "  'experiments': {u'hotfix-http-throttling-v2-bug-1462906': {u'branch': u'hotfix',\n",
       "    u'type': u'normandy-exp'},\n",
       "   u'rollout-release-61-tls-fallback-1-3': {u'branch': u'active',\n",
       "    u'type': u'normandy-prefrollout'}},\n",
       "  'frame_time': None,\n",
       "  'frame_time_sum': None,\n",
       "  'gfx': {u'ContentBackend': u'Skia',\n",
       "   u'D2DEnabled': False,\n",
       "   u'DWriteEnabled': True,\n",
       "   u'adapters': [{u'GPUActive': True,\n",
       "     u'RAM': 1024,\n",
       "     u'description': u'AMD Radeon HD 6700 Series',\n",
       "     u'deviceID': u'0x673e',\n",
       "     u'driver': u'aticfx64 aticfx64 aticfx64 aticfx32 aticfx32 aticfx32 atiumd64 atidxx64 atidxx64 atiumdag atidxx32 atidxx32 atiumdva atiumd6a atitmm64',\n",
       "     u'driverDate': u'2-26-2016',\n",
       "     u'driverVersion': u'15.301.1901.0',\n",
       "     u'subsysID': u'23101787',\n",
       "     u'vendorID': u'0x1002'}],\n",
       "   u'features': {u'advancedLayers': {u'status': u'available'},\n",
       "    u'compositor': u'd3d11',\n",
       "    u'd2d': {u'status': u'available', u'version': u'1.1'},\n",
       "    u'd3d11': {u'blacklisted': False,\n",
       "     u'status': u'available',\n",
       "     u'textureSharing': True,\n",
       "     u'version': 45056,\n",
       "     u'warp': False},\n",
       "    u'gpuProcess': {u'status': u'available'}},\n",
       "   u'monitors': [{u'pseudoDisplay': False,\n",
       "     u'refreshRate': 60,\n",
       "     u'screenHeight': 1200,\n",
       "     u'screenWidth': 1920}]},\n",
       "  'os': {u'installYear': 2015,\n",
       "   u'locale': u'en-US',\n",
       "   u'name': u'Windows_NT',\n",
       "   u'servicePackMajor': 0,\n",
       "   u'servicePackMinor': 0,\n",
       "   u'version': u'6.3',\n",
       "   u'windowsBuildNumber': 9600},\n",
       "  'tab_switch': {u'0': 0, u'1': 1, u'2': 3, u'3': 0}},\n",
       " {'buildId': u'20180807170231',\n",
       "  'checkerboardin': None,\n",
       "  'clientId': u'c428dcca-a0b2-4907-8207-8cc9493f413b',\n",
       "  'composite_time': None,\n",
       "  'content_paint': None,\n",
       "  'content_paint_phase': None,\n",
       "  'experiments': {u'hotfix-http-throttling-v2-bug-1462906': {u'branch': u'hotfix',\n",
       "    u'type': u'normandy-exp'},\n",
       "   u'rollout-release-61-tls-fallback-1-3': {u'branch': u'active',\n",
       "    u'type': u'normandy-prefrollout'}},\n",
       "  'frame_time': None,\n",
       "  'frame_time_sum': None,\n",
       "  'gfx': {u'ContentBackend': u'Skia',\n",
       "   u'D2DEnabled': False,\n",
       "   u'DWriteEnabled': True,\n",
       "   u'adapters': [{u'GPUActive': True,\n",
       "     u'RAM': 2048,\n",
       "     u'description': u'NVIDIA GeForce GT 610',\n",
       "     u'deviceID': u'0x104a',\n",
       "     u'driver': u'C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_ref_pubwu.inf_amd64_2e7fa54192fe16d0\\\\nvldumdx.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_ref_pubwu.inf_amd64_2e7fa54192fe16d0\\\\nvldumdx.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_ref_pubwu.inf_amd64_2e7fa54192fe16d0\\\\nvldumdx.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_ref_pubwu.inf_amd64_2e7fa54192fe16d0\\\\nvldumdx.dll C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_ref_pubwu.inf_amd64_2e7fa54192fe16d0\\\\nvldumd.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_ref_pubwu.inf_amd64_2e7fa54192fe16d0\\\\nvldumd.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_ref_pubwu.inf_amd64_2e7fa54192fe16d0\\\\nvldumd.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_ref_pubwu.inf_amd64_2e7fa54192fe16d0\\\\nvldumd.dll',\n",
       "     u'driverDate': u'10-27-2017',\n",
       "     u'driverVersion': u'23.21.13.8813',\n",
       "     u'subsysID': u'249b1682',\n",
       "     u'vendorID': u'0x10de'}],\n",
       "   u'features': {u'advancedLayers': {u'status': u'available'},\n",
       "    u'compositor': u'd3d11',\n",
       "    u'd2d': {u'status': u'available', u'version': u'1.1'},\n",
       "    u'd3d11': {u'blacklisted': False,\n",
       "     u'status': u'available',\n",
       "     u'textureSharing': True,\n",
       "     u'version': 45056,\n",
       "     u'warp': False},\n",
       "    u'gpuProcess': {u'status': u'available'}},\n",
       "   u'monitors': [{u'pseudoDisplay': False,\n",
       "     u'refreshRate': 60,\n",
       "     u'screenHeight': 1080,\n",
       "     u'screenWidth': 1920}]},\n",
       "  'os': {u'installYear': 2018,\n",
       "   u'locale': u'de-DE',\n",
       "   u'name': u'Windows_NT',\n",
       "   u'servicePackMajor': 0,\n",
       "   u'servicePackMinor': 0,\n",
       "   u'version': u'10.0',\n",
       "   u'windowsBuildNumber': 17134,\n",
       "   u'windowsUBR': 228},\n",
       "  'tab_switch': {u'0': 0, u'1': 1, u'2': 2, u'27': 1, u'39': 0}},\n",
       " {'buildId': u'20180807170231',\n",
       "  'checkerboardin': None,\n",
       "  'clientId': u'4927477e-990f-434b-a713-6836192cada7',\n",
       "  'composite_time': None,\n",
       "  'content_paint': None,\n",
       "  'content_paint_phase': None,\n",
       "  'experiments': {u'hotfix-http-throttling-v2-bug-1462906': {u'branch': u'hotfix',\n",
       "    u'type': u'normandy-exp'},\n",
       "   u'rollout-release-61-tls-fallback-1-3': {u'branch': u'active',\n",
       "    u'type': u'normandy-prefrollout'}},\n",
       "  'frame_time': None,\n",
       "  'frame_time_sum': None,\n",
       "  'gfx': {u'ContentBackend': u'Skia',\n",
       "   u'D2DEnabled': False,\n",
       "   u'DWriteEnabled': True,\n",
       "   u'adapters': [{u'GPUActive': True,\n",
       "     u'RAM': None,\n",
       "     u'description': u'Intel(R) HD Graphics 4400',\n",
       "     u'deviceID': u'0x041e',\n",
       "     u'driver': u'igdumdim64 igd10iumd64 igd10iumd64 igd12umd64 igdumdim32 igd10iumd32 igd10iumd32 igd12umd32',\n",
       "     u'driverDate': u'10-16-2017',\n",
       "     u'driverVersion': u'20.19.15.4835',\n",
       "     u'subsysID': u'00000000',\n",
       "     u'vendorID': u'0x8086'}],\n",
       "   u'features': {u'advancedLayers': {u'status': u'available'},\n",
       "    u'compositor': u'd3d11',\n",
       "    u'd2d': {u'status': u'available', u'version': u'1.1'},\n",
       "    u'd3d11': {u'blacklisted': False,\n",
       "     u'status': u'available',\n",
       "     u'textureSharing': True,\n",
       "     u'version': 45312,\n",
       "     u'warp': False},\n",
       "    u'gpuProcess': {u'status': u'available'}},\n",
       "   u'monitors': [{u'pseudoDisplay': False,\n",
       "     u'refreshRate': 60,\n",
       "     u'screenHeight': 1024,\n",
       "     u'screenWidth': 1280}]},\n",
       "  'os': {u'installYear': 2018,\n",
       "   u'locale': u'es-ES',\n",
       "   u'name': u'Windows_NT',\n",
       "   u'servicePackMajor': 0,\n",
       "   u'servicePackMinor': 0,\n",
       "   u'version': u'10.0',\n",
       "   u'windowsBuildNumber': 17134,\n",
       "   u'windowsUBR': 228},\n",
       "  'tab_switch': {u'0': 0,\n",
       "   u'1': 30,\n",
       "   u'115': 0,\n",
       "   u'19': 1,\n",
       "   u'2': 18,\n",
       "   u'27': 2,\n",
       "   u'3': 19,\n",
       "   u'4': 12,\n",
       "   u'6': 2,\n",
       "   u'80': 2,\n",
       "   u'9': 1}},\n",
       " {'buildId': u'20180807170231',\n",
       "  'checkerboardin': None,\n",
       "  'clientId': u'8103a957-6e70-4b90-8db9-a869c44ec046',\n",
       "  'composite_time': None,\n",
       "  'content_paint': None,\n",
       "  'content_paint_phase': None,\n",
       "  'experiments': {u'hotfix-http-throttling-v2-bug-1462906': {u'branch': u'hotfix',\n",
       "    u'type': u'normandy-exp'},\n",
       "   u'rollout-release-61-tls-fallback-1-3': {u'branch': u'active',\n",
       "    u'type': u'normandy-prefrollout'},\n",
       "   u'searchCohort': {u'branch': u'nov17-2'}},\n",
       "  'frame_time': None,\n",
       "  'frame_time_sum': None,\n",
       "  'gfx': {u'ContentBackend': u'Skia',\n",
       "   u'D2DEnabled': False,\n",
       "   u'DWriteEnabled': True,\n",
       "   u'adapters': [{u'GPUActive': True,\n",
       "     u'RAM': None,\n",
       "     u'description': u'Intel(R) G45/G43 Express Chipset',\n",
       "     u'deviceID': u'0x2e22',\n",
       "     u'driver': u'igdumdx32 igd10umd32',\n",
       "     u'driverDate': u'2-11-2011',\n",
       "     u'driverVersion': u'8.15.10.2302',\n",
       "     u'subsysID': u'02831028',\n",
       "     u'vendorID': u'0x8086'}],\n",
       "   u'features': {u'advancedLayers': {u'noConstantBufferOffsetting': True,\n",
       "     u'status': u'available'},\n",
       "    u'compositor': u'd3d11',\n",
       "    u'd2d': {u'status': u'blacklisted', u'version': u'1.1'},\n",
       "    u'd3d11': {u'blacklisted': False,\n",
       "     u'status': u'available',\n",
       "     u'textureSharing': True,\n",
       "     u'version': 40960,\n",
       "     u'warp': False},\n",
       "    u'gpuProcess': {u'status': u'available'}},\n",
       "   u'monitors': [{u'pseudoDisplay': False,\n",
       "     u'refreshRate': 59,\n",
       "     u'screenHeight': 1050,\n",
       "     u'screenWidth': 1680}]},\n",
       "  'os': {u'installYear': 2011,\n",
       "   u'locale': u'en-US',\n",
       "   u'name': u'Windows_NT',\n",
       "   u'servicePackMajor': 1,\n",
       "   u'servicePackMinor': 0,\n",
       "   u'version': u'6.1',\n",
       "   u'windowsBuildNumber': 7601},\n",
       "  'tab_switch': {u'0': 0,\n",
       "   u'1': 10,\n",
       "   u'115': 2,\n",
       "   u'165': 1,\n",
       "   u'2': 11,\n",
       "   u'27': 2,\n",
       "   u'3': 4,\n",
       "   u'340': 1,\n",
       "   u'39': 1,\n",
       "   u'487': 0,\n",
       "   u'80': 2}}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pings.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2272516"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add two extra steps. The first rewrites the ping to have some\n",
    "# information more easily accessible (like the primary adapter),\n",
    "# and the second step removes any pings that don't have adapter\n",
    "# information.\n",
    "def rewrite_ping(p):\n",
    "    adapters = p.get('gfx', None).get('adapters', None)\n",
    "    if not adapters:\n",
    "        return None\n",
    "    adapter = adapters[0]\n",
    "            \n",
    "    p['adapter'] = adapter\n",
    "            \n",
    "    # Convert the version to a tuple of integers.\n",
    "    #if 'driverVersion' in adapter:\n",
    "    #    p['driverVersion'] = [int(n) for n in adapter['driverVersion'].split('.') if n.isdigit()]\n",
    "    return p\n",
    "\n",
    "def filter_ping(p):\n",
    "    return 'adapter' in p\n",
    "rpings = pings.map(rewrite_ping).filter(filter_ping)\n",
    "rpings = rpings.cache()\n",
    "rpings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent pseudoreplication, let's consider only a single submission for each client. As this step requires a distributed shuffle, it should always be run only after extracting the attributes of interest with *Dataset.select()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = (\n",
    "    rpings\n",
    "    .map(lambda p: (p['clientId'], p))\n",
    "    .reduceByKey(lambda p1, p2: p1)\n",
    "    .map(lambda p: p[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching is fundamental as it allows for an iterative, real-time development workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached = subset.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many pings are we looking at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2118920"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrQualified = cached.filter(lambda p: \"features\" in p[\"gfx\"])\n",
    "wrQualified = wrQualified.filter(lambda p: \"wrQualified\" in p[\"gfx\"][\"features\"])\n",
    "wrQualified.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {u'blocked': 2})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrQualified.map(lambda p: p[\"gfx\"][\"features\"][\"wrQualified\"][\"status\"]).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {u'0x8086': 1, u'X.Org': 1})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrQualified.map(lambda p: p['adapter']['vendorID']).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {u'blocked': 2})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrQualified.map(lambda p: p[\"gfx\"][\"features\"][\"wrQualified\"][\"status\"]).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {u'Linux': 1, u'Windows_NT': 1})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrQualified.map(lambda p: p[\"os\"][\"name\"]).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 101.0 failed 4 times, most recent failure: Lost task 18.3 in stage 101.0 (TID 3534, ip-172-31-4-232.us-west-2.compute.internal, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1041, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1041, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"<ipython-input-66-f8052b31a96f>\", line 1, in <lambda>\nKeyError: 'wrQualified'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1708)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1695)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1695)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1867)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:671)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor122.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1041, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1041, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"<ipython-input-66-f8052b31a96f>\", line 1, in <lambda>\nKeyError: 'wrQualified'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-f8052b31a96f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwrAvailable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gfx\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"wrQualified\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"status\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"available\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwrAvailable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \"\"\"\n\u001b[0;32m-> 1041\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \"\"\"\n\u001b[0;32m-> 1032\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 101.0 failed 4 times, most recent failure: Lost task 18.3 in stage 101.0 (TID 3534, ip-172-31-4-232.us-west-2.compute.internal, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1041, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1041, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"<ipython-input-66-f8052b31a96f>\", line 1, in <lambda>\nKeyError: 'wrQualified'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1708)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1695)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1695)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1867)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:671)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor122.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1041, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1041, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"<ipython-input-66-f8052b31a96f>\", line 1, in <lambda>\nKeyError: 'wrQualified'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "wrAvailable = cached.filter(lambda p: p[\"gfx\"][\"features\"][\"wrQualified\"][\"status\"] == \"available\" )\n",
    "wrAvailable.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrBlocked = cached.filter(lambda p: p[\"gfx\"][\"features\"][\"wrQualified\"][\"status\"] == \"blocked\" )\n",
    "wrBlocked.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100.*wrAvailable.count()/cached.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib2\n",
    "\n",
    "gpu_db = json.load(urllib2.urlopen('https://raw.githubusercontent.com/jrmuizel/gpu-db/master/nvidia.json'))\n",
    "devices = {}\n",
    "for gen in gpu_db['10de'].items():\n",
    "    for chipset in gen[1].items():\n",
    "        for dev in chipset[1]:\n",
    "            #print dev, gen[0]\n",
    "            devices[int(dev,16)] = chipset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 103.0 failed 4 times, most recent failure: Lost task 7.3 in stage 103.0 (TID 3571, ip-172-31-4-232.us-west-2.compute.internal, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 830, in func\n    initial = next(iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1239, in countPartition\n    for obj in iterator:\n  File \"<ipython-input-68-ad0a8c05fdad>\", line 1, in <lambda>\nValueError: invalid literal for int() with base 16: 'AMD HAWAII (DRM 2.50.0, 4.18.3-arch1-1-ARCH, LLVM 6.0.1)'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1708)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1695)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1695)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1867)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:671)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor122.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 830, in func\n    initial = next(iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1239, in countPartition\n    for obj in iterator:\n  File \"<ipython-input-68-ad0a8c05fdad>\", line 1, in <lambda>\nValueError: invalid literal for int() with base 16: 'AMD HAWAII (DRM 2.50.0, 4.18.3-arch1-1-ARCH, LLVM 6.0.1)'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-ad0a8c05fdad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrQualified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"adapter\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"deviceID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountByValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcountByValue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1245\u001b[0m                 \u001b[0mm1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mm1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountPartition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmergeMaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 103.0 failed 4 times, most recent failure: Lost task 7.3 in stage 103.0 (TID 3571, ip-172-31-4-232.us-west-2.compute.internal, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 830, in func\n    initial = next(iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1239, in countPartition\n    for obj in iterator:\n  File \"<ipython-input-68-ad0a8c05fdad>\", line 1, in <lambda>\nValueError: invalid literal for int() with base 16: 'AMD HAWAII (DRM 2.50.0, 4.18.3-arch1-1-ARCH, LLVM 6.0.1)'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1708)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1695)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1695)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1867)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:671)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor122.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 830, in func\n    initial = next(iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1239, in countPartition\n    for obj in iterator:\n  File \"<ipython-input-68-ad0a8c05fdad>\", line 1, in <lambda>\nValueError: invalid literal for int() with base 16: 'AMD HAWAII (DRM 2.50.0, 4.18.3-arch1-1-ARCH, LLVM 6.0.1)'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "wrQualified.map(lambda p: devices[int(p[\"adapter\"][\"deviceID\"],16)]).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrQualified.map(lambda p: devices[int(p[\"adapter\"][\"deviceID\"],16)]).filter(lambda p: p.endswith(\"M\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrQualified.map(lambda p: p[\"gfx\"][\"features\"][\"compositor\"]).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342477"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nv = cached.filter(lambda p: p[\"adapter\"][\"vendorID\"] == \"0x10de\")\n",
    "nv.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147134"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nv10 = nv.filter(lambda p: p[\"os\"][\"name\"] == \"Windows_NT\" and p[\"os\"][\"version\"] == \"10.0\")\n",
    "nv10.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145446"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nv10tesla = nv10.filter(lambda p: int(p[\"adapter\"][\"deviceID\"],16) >= 1000)\n",
    "nv10tesla.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv10teslanom = nv10tesla.filter(lambda p: int(p[\"adapter\"][\"deviceID\"],16) in devices).filter(lambda p: not devices[int(p[\"adapter\"][\"deviceID\"],16)].endswith(\"M\"))\n",
    "nv10teslanom.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {u'0x0f03': 1, u'0x1399': 1, u'0x179c': 12})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nv10tesla.filter(lambda p: int(p[\"adapter\"][\"deviceID\"],16) not in devices).map(lambda p: p['adapter']['deviceID']).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.205095048420894"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100.*nv10teslanom.count()/cached.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nv10blocked = nv10tesla.filter(lambda p: p[\"gfx\"][\"features\"][\"wrQualified\"][\"status\"] == \"blocked\")\n",
    "nv10blocked.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {u'G84GLM': 1,\n",
       "             u'G86GLM': 1,\n",
       "             u'G86M': 1,\n",
       "             u'G92': 1,\n",
       "             u'G92GLM': 1,\n",
       "             u'G94M': 1,\n",
       "             u'G96M': 1,\n",
       "             u'G98M': 3,\n",
       "             u'GF106GLM': 1,\n",
       "             u'GF106M': 2,\n",
       "             u'GF108GLM': 2,\n",
       "             u'GF108M': 5,\n",
       "             u'GF116M': 3,\n",
       "             u'GF117M': 5,\n",
       "             u'GF119M': 3,\n",
       "             u'GK104': 3,\n",
       "             u'GK104GLM': 2,\n",
       "             u'GK104M': 3,\n",
       "             u'GK106': 3,\n",
       "             u'GK106GLM': 4,\n",
       "             u'GK106M': 7,\n",
       "             u'GK107': 2,\n",
       "             u'GK107GLM': 3,\n",
       "             u'GK107M': 7,\n",
       "             u'GK208BM': 2,\n",
       "             u'GK208M': 2,\n",
       "             u'GM107': 2,\n",
       "             u'GM107GLM': 12,\n",
       "             u'GM107M': 8,\n",
       "             u'GM108M': 5,\n",
       "             u'GM200': 2,\n",
       "             u'GM204': 10,\n",
       "             u'GM204M': 17,\n",
       "             u'GM206': 4,\n",
       "             u'GM206M': 1,\n",
       "             u'GP102': 4,\n",
       "             u'GP104': 12,\n",
       "             u'GP104M': 33,\n",
       "             u'GP106': 7,\n",
       "             u'GP106M': 31,\n",
       "             u'GP107': 1,\n",
       "             u'GP107M': 11,\n",
       "             u'GP108M': 1,\n",
       "             u'GT215M': 1,\n",
       "             u'GT216GLM': 2,\n",
       "             u'GT216M': 5,\n",
       "             u'GT218M': 7})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nv10blocked.map(lambda p: devices[int(p[\"adapter\"][\"deviceID\"],16)]).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {u'basic': 123, u'd3d11': 2622, u'webrender': 664})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrExperiment = cached.filter(lambda p: \"experiments\" in p and p[\"experiments\"]).filter(lambda p: \"prefflip-webrender-v1-1-1474484\" in p[\"experiments\"])\n",
    "wrExperiment.map(lambda p: p[\"gfx\"][\"features\"][\"compositor\"]).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrExperiment = wrExperiment.filter(lambda p: p[\"gfx\"][\"features\"][\"wrQualified\"][\"status\"] == \"available\")\n",
    "wrExperiment = wrExperiment.filter(lambda p: len(p[\"gfx\"][\"monitors\"]) == 1 and p[\"gfx\"][\"monitors\"][0][\"refreshRate\"] == 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {u'control': 495, u'treatment': 472})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrExperiment.map(lambda p: p[\"experiments\"][\"prefflip-webrender-v1-1-1474484\"][\"branch\"]).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment = wrExperiment.filter(lambda p: p[\"experiments\"][\"prefflip-webrender-v1-1-1474484\"][\"branch\"] == \"treatment\")\n",
    "control = wrExperiment.filter(lambda p: p[\"experiments\"][\"prefflip-webrender-v1-1-1474484\"][\"branch\"] == \"control\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrt = qTreatment.filter(lambda p: p[\"gfx\"][\"features\"][\"compositor\"] == 'd3d11').filter(lambda p: 'webrender' in p[\"gfx\"][\"features\"])\n",
    "#wrt.map(lambda p: p[\"gfx\"][\"features\"][\"webrender\"][\"status\"]).countByValue()\n",
    "#wrt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {u'basic': 196,\n",
       "             u'd3d11': 4122,\n",
       "             u'none': 1,\n",
       "             u'opengl': 1,\n",
       "             u'webrender': 341})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached.filter(lambda p: p['checkerboardin']).map(lambda p: p[\"gfx\"][\"features\"][\"compositor\"]).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {None: 3, False: 2, True: 336})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked = cached.filter(lambda p: p['checkerboardin']).filter(lambda p: p[\"gfx\"][\"features\"][\"compositor\"] == \"webrender\")\n",
    "checked.map(lambda p: p[\"gfx\"][\"D2DEnabled\"]).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {u'basic': 8, u'd3d11': 193, u'webrender': 271})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment.map(lambda p: p[\"gfx\"][\"features\"][\"compositor\"]).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'adapter': {u'GPUActive': True,\n",
       "   u'RAM': 2048,\n",
       "   u'description': u'NVIDIA GeForce GTX 660',\n",
       "   u'deviceID': u'0x11c0',\n",
       "   u'driver': u'C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_dispi.inf_amd64_38c9bee769f9ef1f\\\\nvldumdx.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_dispi.inf_amd64_38c9bee769f9ef1f\\\\nvldumdx.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_dispi.inf_amd64_38c9bee769f9ef1f\\\\nvldumdx.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_dispi.inf_amd64_38c9bee769f9ef1f\\\\nvldumdx.dll C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_dispi.inf_amd64_38c9bee769f9ef1f\\\\nvldumd.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_dispi.inf_amd64_38c9bee769f9ef1f\\\\nvldumd.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_dispi.inf_amd64_38c9bee769f9ef1f\\\\nvldumd.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_dispi.inf_amd64_38c9bee769f9ef1f\\\\nvldumd.dll',\n",
       "   u'driverDate': u'5-7-2018',\n",
       "   u'driverVersion': u'24.21.13.9764',\n",
       "   u'subsysID': u'354e1458',\n",
       "   u'vendorID': u'0x10de'},\n",
       "  'buildId': u'20180802100128',\n",
       "  'checkerboardin': None,\n",
       "  'clientId': u'49465b34-1fd4-468d-af82-50fc272deb47',\n",
       "  'composite_time': {u'bucket_count': 50,\n",
       "   u'histogram_type': 0,\n",
       "   u'range': [1, 1000],\n",
       "   u'sum': 1151,\n",
       "   u'values': {u'0': 1812,\n",
       "    u'1': 459,\n",
       "    u'11': 1,\n",
       "    u'152': 1,\n",
       "    u'171': 0,\n",
       "    u'2': 151,\n",
       "    u'20': 1,\n",
       "    u'23': 1,\n",
       "    u'3': 18,\n",
       "    u'4': 6,\n",
       "    u'5': 3,\n",
       "    u'53': 1,\n",
       "    u'8': 1,\n",
       "    u'9': 1}},\n",
       "  'content_paint': {u'0': 14,\n",
       "   u'1': 410,\n",
       "   u'10': 6,\n",
       "   u'11': 3,\n",
       "   u'12': 1,\n",
       "   u'14': 3,\n",
       "   u'16': 2,\n",
       "   u'2': 265,\n",
       "   u'23': 1,\n",
       "   u'26': 1,\n",
       "   u'29': 1,\n",
       "   u'3': 132,\n",
       "   u'33': 0,\n",
       "   u'4': 27,\n",
       "   u'5': 36,\n",
       "   u'6': 19,\n",
       "   u'7': 5,\n",
       "   u'8': 5,\n",
       "   u'9': 5},\n",
       "  'content_paint_phase': {u'dl': {u'bucket_count': 10,\n",
       "    u'histogram_type': 1,\n",
       "    u'range': [1, 100],\n",
       "    u'sum': 82,\n",
       "    u'values': {u'0': 0, u'1': 4, u'63': 1, u'75': 0}},\n",
       "   u'flb': {u'bucket_count': 10,\n",
       "    u'histogram_type': 1,\n",
       "    u'range': [1, 100],\n",
       "    u'sum': 5,\n",
       "    u'values': {u'0': 2, u'1': 3, u'13': 0}},\n",
       "   u'fr': {u'bucket_count': 10,\n",
       "    u'histogram_type': 1,\n",
       "    u'range': [1, 100],\n",
       "    u'sum': 0,\n",
       "    u'values': {u'0': 5, u'1': 0}},\n",
       "   u'r': {u'bucket_count': 10,\n",
       "    u'histogram_type': 1,\n",
       "    u'range': [1, 100],\n",
       "    u'sum': 385,\n",
       "    u'values': {u'100': 0, u'13': 0, u'26': 1, u'75': 2, u'88': 2}}},\n",
       "  'experiments': {u'prefflip-webrender-v1-1-1474484': {u'branch': u'control',\n",
       "    u'type': u'normandy-exp'}},\n",
       "  'frame_time': {u'103': 136,\n",
       "   u'1059': 1,\n",
       "   u'11': 4,\n",
       "   u'120': 10,\n",
       "   u'1237': 0,\n",
       "   u'13': 5,\n",
       "   u'140': 1,\n",
       "   u'15': 20,\n",
       "   u'164': 6,\n",
       "   u'18': 21,\n",
       "   u'192': 2,\n",
       "   u'21': 9,\n",
       "   u'25': 7,\n",
       "   u'262': 3,\n",
       "   u'29': 8,\n",
       "   u'34': 17,\n",
       "   u'357': 1,\n",
       "   u'40': 24,\n",
       "   u'47': 12,\n",
       "   u'55': 6,\n",
       "   u'64': 10,\n",
       "   u'75': 55,\n",
       "   u'777': 1,\n",
       "   u'8': 0,\n",
       "   u'88': 555,\n",
       "   u'9': 3},\n",
       "  'frame_time_sum': 84371,\n",
       "  'gfx': {u'ContentBackend': u'Skia',\n",
       "   u'D2DEnabled': True,\n",
       "   u'DWriteEnabled': True,\n",
       "   u'adapters': [{u'GPUActive': True,\n",
       "     u'RAM': 2048,\n",
       "     u'description': u'NVIDIA GeForce GTX 660',\n",
       "     u'deviceID': u'0x11c0',\n",
       "     u'driver': u'C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_dispi.inf_amd64_38c9bee769f9ef1f\\\\nvldumdx.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_dispi.inf_amd64_38c9bee769f9ef1f\\\\nvldumdx.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_dispi.inf_amd64_38c9bee769f9ef1f\\\\nvldumdx.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_dispi.inf_amd64_38c9bee769f9ef1f\\\\nvldumdx.dll C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_dispi.inf_amd64_38c9bee769f9ef1f\\\\nvldumd.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_dispi.inf_amd64_38c9bee769f9ef1f\\\\nvldumd.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_dispi.inf_amd64_38c9bee769f9ef1f\\\\nvldumd.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nv_dispi.inf_amd64_38c9bee769f9ef1f\\\\nvldumd.dll',\n",
       "     u'driverDate': u'5-7-2018',\n",
       "     u'driverVersion': u'24.21.13.9764',\n",
       "     u'subsysID': u'354e1458',\n",
       "     u'vendorID': u'0x10de'}],\n",
       "   u'features': {u'advancedLayers': {u'status': u'available'},\n",
       "    u'compositor': u'd3d11',\n",
       "    u'd2d': {u'status': u'available', u'version': u'1.1'},\n",
       "    u'd3d11': {u'blacklisted': False,\n",
       "     u'status': u'available',\n",
       "     u'textureSharing': True,\n",
       "     u'version': 45056,\n",
       "     u'warp': False},\n",
       "    u'gpuProcess': {u'status': u'available'},\n",
       "    u'webrender': {u'status': u'opt-in'},\n",
       "    u'wrQualified': {u'status': u'available'}},\n",
       "   u'monitors': [{u'pseudoDisplay': False,\n",
       "     u'refreshRate': 60,\n",
       "     u'screenHeight': 1080,\n",
       "     u'screenWidth': 1920}]},\n",
       "  'osName': u'Windows_NT',\n",
       "  'tab_switch': None}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrQualified.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271, 476)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrEnabled = treatment.filter(lambda p: p[\"gfx\"][\"features\"][\"compositor\"] == \"webrender\")\n",
    "wrDisabled = control.filter(lambda p: p[\"gfx\"][\"features\"][\"compositor\"] == \"d3d11\")\n",
    "wrEnabled.count(), wrDisabled.count()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
